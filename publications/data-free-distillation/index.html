<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Large-Scale Generative Data-Free Distillation - Liangchen Luo</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Large-Scale Generative Data-Free Distillation" />
<meta property="og:description" content="Abstract Knowledge distillation is one of the most popular and effective techniques for knowledge transfer, model compression and semi-supervised learning. Most existing distillation approaches require the access to original or augmented training samples. But this can be problematic in practice due to privacy, proprietary and availability concerns. Recent work has put forward some methods to tackle this problem, but they are either highly time-consuming or unable to scale to large datasets." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.luolc.com/publications/data-free-distillation/" /><meta property="article:section" content="publications" />
<meta property="article:published_time" content="2020-12-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-12-10T00:00:00+00:00" />


		<meta itemprop="name" content="Large-Scale Generative Data-Free Distillation">
<meta itemprop="description" content="Abstract Knowledge distillation is one of the most popular and effective techniques for knowledge transfer, model compression and semi-supervised learning. Most existing distillation approaches require the access to original or augmented training samples. But this can be problematic in practice due to privacy, proprietary and availability concerns. Recent work has put forward some methods to tackle this problem, but they are either highly time-consuming or unable to scale to large datasets."><meta itemprop="datePublished" content="2020-12-10T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-12-10T00:00:00+00:00" />
<meta itemprop="wordCount" content="164">
<meta itemprop="keywords" content="" />
		<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Large-Scale Generative Data-Free Distillation"/>
<meta name="twitter:description" content="Abstract Knowledge distillation is one of the most popular and effective techniques for knowledge transfer, model compression and semi-supervised learning. Most existing distillation approaches require the access to original or augmented training samples. But this can be problematic in practice due to privacy, proprietary and availability concerns. Recent work has put forward some methods to tackle this problem, but they are either highly time-consuming or unable to scale to large datasets."/>

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">

	<link rel="shortcut icon" sizes="16x16 24x24 32x32 48x48 64x64" href="/favicon.ico">
		
		
<script>
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-97612757-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [ ['$','$'], ["\\(","\\)"] ],
				processEscapes: true
			}
		});
	</script>
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Liangchen Luo" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Liangchen Luo</div>
					<div class="logo__tagline">A fool living in the amazing world</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			
			<a class="menu__link" href="/assets/other/CV-LiangchenLuo.pdf">
				
				<span class="menu__text">Curriculum Vitae</span>
				
			</a>
		</li>
		<li class="menu__item menu__item--active">
			
			<a class="menu__link" href="/publications/">
				
				<span class="menu__text">Publications</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Large-Scale Generative Data-Free Distillation</h1>
			
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 577.545 577.545">
		<g>
			<path d="M245.531,245.532c4.893-4.896,11.42-7.589,18.375-7.589s13.482,2.696,18.375,7.589l49.734,49.734    c1.723,1.72,4.058,2.689,6.49,2.689s4.771-0.967,6.49-2.689l49.733-49.734c1.724-1.72,2.69-4.058,2.69-6.49    c0-2.433-0.967-4.771-2.69-6.49l-49.733-49.734c-21.668-21.662-50.469-33.589-81.093-33.589s-59.425,11.928-81.093,33.586    L33.602,332.022C11.934,353.69,0,382.494,0,413.128c0,30.637,11.934,59.432,33.605,81.084l49.731,49.73    c21.65,21.668,50.447,33.603,81.081,33.603s59.438-11.935,81.108-33.603l84.083-84.082c2.705-2.705,3.448-6.803,1.869-10.285    c-1.496-3.295-4.776-5.386-8.356-5.386c-0.205,0-0.407,0.007-0.615,0.021c-2.959,0.199-5.958,0.297-8.917,0.297    c-23.354,0-46.322-6.208-66.417-17.956c-1.444-0.844-3.042-1.254-4.629-1.254c-2.375,0-4.725,0.921-6.494,2.689l-53.238,53.238    c-4.902,4.901-11.426,7.604-18.372,7.604c-6.949,0-13.479-2.699-18.381-7.604l-49.734-49.734    c-4.908-4.896-7.61-11.411-7.616-18.348c-0.003-6.953,2.699-13.489,7.616-18.406L245.531,245.532z"/>
			<path d="M543.942,83.324L494.208,33.59C472.556,11.931,443.762,0,413.128,0s-59.438,11.928-81.105,33.587l-84.086,84.119    c-2.705,2.705-3.448,6.806-1.867,10.288c1.497,3.292,4.777,5.382,8.354,5.382c0.205,0,0.413-0.006,0.621-0.021    c2.987-0.202,6.013-0.303,9-0.303c23.4,0,46.316,6.206,66.274,17.947c1.45,0.854,3.057,1.267,4.65,1.267    c2.375,0,4.725-0.921,6.494-2.689l53.274-53.274c4.893-4.896,11.42-7.589,18.375-7.589s13.482,2.696,18.375,7.589l49.734,49.734    c10.123,10.135,10.123,26.634-0.003,36.775L332.017,332.014c-4.894,4.905-11.408,7.604-18.348,7.604    c-6.956,0-13.495-2.702-18.415-7.61l-49.723-49.725c-1.723-1.72-4.057-2.69-6.49-2.69c-2.433,0-4.771,0.967-6.49,2.69    l-49.734,49.734c-3.586,3.586-3.586,9.397,0,12.983l49.734,49.734c21.668,21.668,50.469,33.602,81.093,33.602    c30.625,0,59.426-11.934,81.094-33.602l149.205-149.206c21.668-21.658,33.603-50.462,33.603-81.102S565.61,104.983,543.942,83.324    z"/>
		</g>
	</svg>
	<ul class="tags__list">
	<li class="tags__item"><a class="tags__link btn" href="https://arxiv.org/abs/2012.05578" rel="tag">arXiv</a></li>
	<li class="tags__item"><a class="tags__link btn" href="https://www.luolc.com/publications/data-free-distillation/#bibtex" rel="tag">bib</a></li>
</ul>
</div>
			<div class="pub__meta"><b>Liangchen Luo</b>, Mark Sandler, Zi Lin, Andrey Zhmoginov, Andrew Howard.<br>
<em>arXiv preprint</em>.
2020.
			</div>
		</header><div class="content post__content clearfix">
			<h2 id="abstract">Abstract</h2>
<p>Knowledge distillation is one of the most popular and effective techniques for knowledge transfer, model compression and semi-supervised learning.
Most existing distillation approaches require the access to original or augmented training samples.
But this can be problematic in practice due to privacy, proprietary and availability concerns.
Recent work has put forward some methods to tackle this problem, but they are either highly time-consuming or unable to scale to large datasets.
To this end, we propose a new method to train a generative image model by leveraging the intrinsic normalization layers&rsquo; statistics of the trained teacher network.
This enables us to build an ensemble of generators without training data that can efficiently produce substitute inputs for subsequent distillation.
The proposed method pushes forward the data-free distillation performance on CIFAR-10 and CIFAR-100 to 95.02% and 77.02% respectively.
Furthermore, we are able to scale it to ImageNet dataset, which to the best of our knowledge, has never been done using generative models in a data-free setting.</p>

		</div>








<h2 id="bibtex">BibTex</h2>
<pre><code>@article{Luo2020DataFreeDistill,
  author = {Luo, Liangchen and Sandler, Mark and Lin, Zi and Zhmoginov, Andrey and Howard, Andrew},
  title = {Large-Scale Generative Data-Free Distillation},
  journal = {arXiv preprint arXiv:2012.05578},
  year = {2020}
}
</code></pre>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/publications/cellular-automata/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Image Segmentation via Cellular Automata</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/publications/query-modularized-detection/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Bridging the Gap Between Object Detection and User Intent via Query-Modulation</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2015-2025 Liangchen Luo.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> based on <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="/js/custom.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>