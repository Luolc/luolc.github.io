<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Adaptive Gradient Methods with Dynamic Bound of Learning Rate</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta name="generator" content="Hugo 0.60.0" />
	<meta property="og:title" content="Adaptive Gradient Methods with Dynamic Bound of Learning Rate" />
<meta property="og:description" content="Abstract Adaptive optimization methods such as AdaGrad, RMSProp and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with Sgd or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.luolc.com/publications/adabound/" />
<meta property="article:published_time" content="2019-05-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-05-08T00:00:00+00:00" />

	<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Adaptive Gradient Methods with Dynamic Bound of Learning Rate"/>
<meta name="twitter:description" content="Abstract Adaptive optimization methods such as AdaGrad, RMSProp and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with Sgd or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods."/>

	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="shortcut icon" sizes="16x16 24x24 32x32 48x48 64x64" href="/favicon.ico">
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-97612757-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [ ['$','$'], ["\\(","\\)"] ],
				processEscapes: true
			}
		});
	</script>
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo" role="banner">
			<a class="logo__link" href="/" title="Liangchen Luo" rel="home">
				<div class="logo__title">Liangchen Luo</div>
				<div class="logo__tagline">A fool living in the amazing world</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			
			<a class="menu__link" href="/assets/other/CV.pdf">Curriculum Vitae</a>
		</li>
		<li class="menu__item">
			
			<a class="menu__link" href="/publications/">Publications</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<section class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Adaptive Gradient Methods with Dynamic Bound of Learning Rate</h1>
			
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 577.545 577.545">
		<g>
			<path d="M245.531,245.532c4.893-4.896,11.42-7.589,18.375-7.589s13.482,2.696,18.375,7.589l49.734,49.734    c1.723,1.72,4.058,2.689,6.49,2.689s4.771-0.967,6.49-2.689l49.733-49.734c1.724-1.72,2.69-4.058,2.69-6.49    c0-2.433-0.967-4.771-2.69-6.49l-49.733-49.734c-21.668-21.662-50.469-33.589-81.093-33.589s-59.425,11.928-81.093,33.586    L33.602,332.022C11.934,353.69,0,382.494,0,413.128c0,30.637,11.934,59.432,33.605,81.084l49.731,49.73    c21.65,21.668,50.447,33.603,81.081,33.603s59.438-11.935,81.108-33.603l84.083-84.082c2.705-2.705,3.448-6.803,1.869-10.285    c-1.496-3.295-4.776-5.386-8.356-5.386c-0.205,0-0.407,0.007-0.615,0.021c-2.959,0.199-5.958,0.297-8.917,0.297    c-23.354,0-46.322-6.208-66.417-17.956c-1.444-0.844-3.042-1.254-4.629-1.254c-2.375,0-4.725,0.921-6.494,2.689l-53.238,53.238    c-4.902,4.901-11.426,7.604-18.372,7.604c-6.949,0-13.479-2.699-18.381-7.604l-49.734-49.734    c-4.908-4.896-7.61-11.411-7.616-18.348c-0.003-6.953,2.699-13.489,7.616-18.406L245.531,245.532z"/>
			<path d="M543.942,83.324L494.208,33.59C472.556,11.931,443.762,0,413.128,0s-59.438,11.928-81.105,33.587l-84.086,84.119    c-2.705,2.705-3.448,6.806-1.867,10.288c1.497,3.292,4.777,5.382,8.354,5.382c0.205,0,0.413-0.006,0.621-0.021    c2.987-0.202,6.013-0.303,9-0.303c23.4,0,46.316,6.206,66.274,17.947c1.45,0.854,3.057,1.267,4.65,1.267    c2.375,0,4.725-0.921,6.494-2.689l53.274-53.274c4.893-4.896,11.42-7.589,18.375-7.589s13.482,2.696,18.375,7.589l49.734,49.734    c10.123,10.135,10.123,26.634-0.003,36.775L332.017,332.014c-4.894,4.905-11.408,7.604-18.348,7.604    c-6.956,0-13.495-2.702-18.415-7.61l-49.723-49.725c-1.723-1.72-4.057-2.69-6.49-2.69c-2.433,0-4.771,0.967-6.49,2.69    l-49.734,49.734c-3.586,3.586-3.586,9.397,0,12.983l49.734,49.734c21.668,21.668,50.469,33.602,81.093,33.602    c30.625,0,59.426-11.934,81.094-33.602l149.205-149.206c21.668-21.658,33.603-50.462,33.603-81.102S565.61,104.983,543.942,83.324    z"/>
		</g>
	</svg>
	<ul class="tags__list">
	<li class="tags__item"><a class="tags__link btn" href="https://arxiv.org/abs/1902.09843" rel="tag">arXiv</a></li>
	<li class="tags__item"><a class="tags__link btn" href="https://www.luolc.com/publications/adabound/#bibtex" rel="tag">bib</a></li>
	<li class="tags__item"><a class="tags__link btn" href="https://github.com/Luolc/AdaBound" rel="tag">code</a></li>
	<li class="tags__item"><a class="tags__link btn" href="https://openreview.net/forum?id=Bkg3g2R9FX" rel="tag">open review</a></li>
	<li class="tags__item"><a class="tags__link btn" href="/assets/research/adabound/ICLR19-AdaBound-poster.pdf" rel="tag">poster</a></li>
	<li class="tags__item"><a class="tags__link btn" href="/assets/research/adabound/slides-AdaptiveGradientMethodsAndBeyond.pdf" rel="tag">slides</a></li>
</ul>
</div>
			<div class="pub__meta"><b>Liangchen Luo*</b>, Yuanhao Xiong*, Yan Liu, Xu Sun.<br>In
<em>Proceedings of the 7th International Conference on Learning Representations</em>, New Orleans, Louisiana.
2019.
			</div>
		</header><div class="content post__content clearfix">
			<h2 id="abstract">Abstract</h2>
<p>Adaptive optimization methods such as <span style="font-variant: small-caps">AdaGrad</span>, <span style="font-variant: small-caps">RMSProp</span> and <span style="font-variant: small-caps">Adam</span> have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates.
Though prevailing, they are observed to generalize poorly compared with <span style="font-variant: small-caps">Sgd</span> or even fail to converge due to unstable and extreme learning rates.
Recent work has put forward some algorithms such as <span style="font-variant: small-caps">AMSGrad</span> to tackle this issue but they failed to achieve considerable improvement over existing methods.
In our paper, we demonstrate that extreme learning rates can lead to poor performance.
We provide new variants of <span style="font-variant: small-caps">Adam</span> and <span style="font-variant: small-caps">AMSGrad</span>, called <span style="font-variant: small-caps">AdaBound</span> and <span style="font-variant: small-caps">AMSBound</span> respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to <span style="font-variant: small-caps">Sgd</span> and give a theoretical proof of convergence.
We further conduct experiments on various popular tasks and models, which is often insufficient in previous work.
Experimental results show that new variants can eliminate the generalization gap between adaptive methods and <span style="font-variant: small-caps">Sgd</span> and maintain higher learning speed early in training at the same time.
Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks.
The implementation of the algorithm can be found at <a href="https://github.com/Luolc/AdaBound">https://github.com/Luolc/AdaBound</a>.</p>
<h2 id="tldr-a-faster-and-better-optimizer-with-highly-robust-performance">TL;DR: A Faster And Better Optimizer with Highly Robust Performance</h2>
<figure style="text-align: center">
	<img src="/assets/research/adabound/adabound-banner.png"/>
</figure>
<h2 id="background">Background</h2>
<p>Since proposed in 1950s, <span style="font-variant: small-caps">Sgd</span> (<a href="https://projecteuclid.org/euclid.aoms/1177729586">Robbins &amp; Monro, 1951</a>)
has performed well across many deep learning applications in spite of its simplicity.
However, there is a disadvantage of <span style="font-variant: small-caps">Sgd</span> that it scales the gradient uniformly in all directions,
which may lead to limited training speed as well as poor performance when the training data are sparse.
To address this problem, recent work has proposed a variety of <em>adaptive</em> methods that scale the gradient by square
roots of some form of the average of the squared values of past gradients.
Examples include <span style="font-variant: small-caps">Adam</span> (<a href="https://arxiv.org/abs/1412.6980">Kingma &amp; Lei Ba, 2015</a>),
<span style="font-variant: small-caps">AdaGrad</span> (<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Duchi et al., 2011</a>)
and <span style="font-variant: small-caps">RMSprop</span> (<a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Tieleman &amp; Hinton, 2012</a>).
<span style="font-variant: small-caps">Adam</span> in particular has become the default algorithm leveraged across many deep learning frameworks due
to its rapid training speed.</p>
<p>Despite their popularity, the generalization ability and out-of-sample behavior of these adaptive methods are likely
worse than their non-adaptive counterparts.
They often display faster progress in the initial portion of the training, but their performance quickly plateaus on
the unseen data (development/test set) (<a href="https://arxiv.org/abs/1705.08292">Wilson et al., 2017</a>).
Indeed, the optimizer is chosen as <span style="font-variant: small-caps">Sgd</span> in several recent state-of-the-art works in NLP and CV,
wherein these instances <span style="font-variant: small-caps">Sgd</span> does perform better than adaptive methods.
Some researchers devoted to solving this problem in some latest studies.
For instance, in the best paper of ICLR 2018 (<a href="https://openreview.net/pdf?id=ryQu7f-RZ">Reddi et al., 2018</a>),
the authors proposed a variant of <span style="font-variant: small-caps">Adam</span> called <span style="font-variant: small-caps">AMSGrad</span>, hoping to solve this problem.
However, they only provided a theoretical proof of convergence while other researchers found that a considerable
performance gap still exists between <span style="font-variant: small-caps">AMSGrad</span> and <span style="font-variant: small-caps">Sgd</span>
(<a href="https://arxiv.org/abs/1712.07628">Keskar &amp; Socher, 2017</a>; <a href="https://arxiv.org/abs/1808.02941">Chen et al., 2018</a>).</p>
<p>All of us are hoping to train models faster and better.
But it seems that we are not able to achieve this easily by now, or at least we have to fine-tune the hyperparameters
of optimizers very carefully.
<strong>A new optimizer that is as fast as <span style="font-variant: small-caps">Adam</span> and as good as <span style="font-variant: small-caps">Sgd</span> is well required by the
community.</strong></p>
<h2 id="extreme-learning-rates-are-evil">Extreme Learning Rates Are Evil</h2>
<p>The first attempt to systematically compare the performances of popular adaptive and non-adaptive optimizers is
<a href="https://arxiv.org/abs/1705.08292">Wilson et al. (2017)</a>, which formally pointed out the performance issue of adaptive methods.
The authors speculated that the lack of generalization ability of adaptive methods may stem from unstable and extreme
learning rates.
However, they did not provide further theoretical analysis or empirical study on the hypothesis.</p>
<p>Along this direction, we conduct a preliminary experiment by sampling learning rates of several weights and biases of
ResNet-34 on CIFAR-10 using <span style="font-variant: small-caps">Adam</span>.</p>
<figure style="text-align: center">
	<img src="/assets/research/adabound/sampled-lr.png"/>
	<figcaption style="text-align: center">
		<p style="display: inline-block; text-align: left;">
  Figure 1: Learning rates of sampled parameters.
  Each cell contains a value obtained by conducting a logarithmic operation on the learning rate.
  The lighter cell stands for the smaller learning rate.
</p>
	</figcaption>
</figure>
<p>We find that when the model is close to convergence, learning rates are composed of tiny ones less than 0.01
as well as huge ones greater than 1000.
This observation show that there are indeed learning rates which are too large or too small in the final stage of the
training process.
But insofar we still have two doubts: (1) does the tiny learning rate really do harm to the convergence of
<span style="font-variant: small-caps">Adam</span>? (2) as the (actual) learning rate highly depends on the initial step size, can we use a
relatively larger initial step size to get rid of too small learning rates?</p>
<p>We thereby go further into the theoretical aspect and prove that:</p>
<p><strong>Theorem 3.</strong> <em>For any constant $\beta_1, \beta_2 \in [0, 1)$ such that $\beta_1 &lt; \sqrt{\beta_2}$, there is
a stochastic convex optimization problem where for any initial step size $\alpha$, <span style="font-variant: small-caps">Adam</span> does
not converge to the optimal solution.</em></p>
<p>This result illustrates the potential bad impact of extreme learning rates and <strong>algorithms are unlikely to achieve
good generalization ability without solving this problem</strong>.
We need to somehow restrict the learning rates of <span style="font-variant: small-caps">Adam</span> at the final stage of training.</p>
<h2 id="applying-dynamic-bound-on-learning-rates">Applying Dynamic Bound on Learning Rates</h2>
<p>To address the problem, we need develop a new variant of optimization methods.
Our aim is to devise a strategy that combines the benefits of adaptive methods, viz. fast initial progress, and
the good final generalization properties of <span style="font-variant: small-caps">Sgd</span>.
Intuitively, we would like to construct an algorithm that behaves like adaptive methods early in training and
like <span style="font-variant: small-caps">Sgd</span> at the end.</p>
<p>We therefore propose the variants of <span style="font-variant: small-caps">Adam</span> and <span style="font-variant: small-caps">AMSGrad</span>, called <span style="font-variant: small-caps">AdaBound</span>
and <span style="font-variant: small-caps">AMSBound</span> by applying dynamic bound on learning rates.
Inspired by <em>gradient clipping</em>, a popular technique in practice that clips the gradients larger than a threshold to
avoid gradient explosion, we may also employ clipping on learning rates in <span style="font-variant: small-caps">Adam</span> to propose
<span style="font-variant: small-caps">AdaBound</span>.
Consider applying the following operation in <span style="font-variant: small-caps">Adam</span>
$$ \mathrm{Clip}(\alpha / \sqrt{V_t}, \eta_l, \eta_u),$$
which clips the learning rate element-wisely such that the output is constrained to be within the lower bound $\eta_l$
and the upper bound $\eta_u$.
It follows that <span style="font-variant: small-caps">Sgd(M)</span> with the step size $\alpha^*$ can be considered as the case where
$\eta_l = \eta_u = \alpha^*$.
As for <span style="font-variant: small-caps">Adam</span>, $\eta_l = 0$ and $\eta_u = \infty$.
To achieve a gradual transformation from <span style="font-variant: small-caps">Adam</span> to <span style="font-variant: small-caps">Sgd</span>, we can employ $\eta_l$ and
$\eta_u$ as functions of $t$ instead of constant values, where $\eta_l(t)$ is a non-decreasing function that starts
from $0$ as $t = 0$ and converges to $\alpha^*$ asymptotically; and $\eta_u(t)$ is a non-increasing function that
starts from $\infty$ as $t = 0$ and also converges to $\alpha^*$ asymptotically.
In this setting, <span style="font-variant: small-caps">AdaBound</span> behaves just like <span style="font-variant: small-caps">Adam</span> at the beginning as the bounds
have very little impact on learning rates, and it gradually transforms to <span style="font-variant: small-caps">Sgd(M)</span> as the bounds
become more and more restricted.
The <span style="font-variant: small-caps">AMSBound</span> optimizer is obtained by employing similar approach on <span style="font-variant: small-caps">AMSGrad</span>.</p>
<h2 id="experiment-results">Experiment Results</h2>
<p>We conduct empirical study of different models to compare new variants with popular methods including:
<span style="font-variant: small-caps">Sgd(M)</span>, <span style="font-variant: small-caps">AdaGrad</span>, <span style="font-variant: small-caps">Adam</span> and <span style="font-variant: small-caps">AMSGrad</span>.
Here we just provide several learning curve figures for a direct and shallow illustration.
Please check the paper for the details of the experiment settings and further analysis.</p>
<figure style="text-align: center">
	<img src="/assets/research/adabound/mnist-results.png"/>
</figure>
<hr>
<figure style="text-align: center">
	<img src="/assets/research/adabound/cifar10-results.png"/>
</figure>
<hr>
<figure style="text-align: center">
	<img src="/assets/research/adabound/ptb-lm-results.png"/>
</figure>
<h2 id="robust-performance">Robust Performance</h2>
<p>We also find an interesting property of <span style="font-variant: small-caps">AdaBound</span>: It is not very sensitive to the hyperparameters,
especially compared with <span style="font-variant: small-caps">Sgd(M)</span>.
This result is very surprising and exciting, for it may translate to a robust performance of <span style="font-variant: small-caps">AdaBound</span>.
We may spent less time fine-tuning the damned hyperparameters by using this lovely new optimizer!
In common cases, a default final learning rate of <code>0.1</code> can achieve relatively good and stable results on unseen data.</p>
<p>Despite of its robust performance, we still have to state that, <strong>there is no silver bullet</strong>.
It does not mean that you will be free from tuning hyperparameters once using <span style="font-variant: small-caps">AdaBound</span>.
The performance of a model depends on so many things including the task, the model structure, the distribution of data,
and etc.
<strong>You still need to decide what hyperparameters to use based on your specific situation, but you may probably use much
less time than before!</strong></p>
<figure style="text-align: center">
	<img src="/assets/research/adabound/different-alpha.png"/>
</figure>
<h2 id="code-and-demo">Code and Demo</h2>
<p>We release the code of <span style="font-variant: small-caps">AdaBound</span> on <a href="https://github.com/Luolc/AdaBound/blob/master/demos/cifar10/visualization.ipynb">GitHub</a>, built on PyTorch.
You can install <span style="font-variant: small-caps">AdaBound</span> easily via <code>pip</code> or directly copying and pasting the source code into your
project.
Besides, thanks to the awesome work by the GitHub team and the Jupyter team, the Jupyter notebook (<code>.ipynb</code>)
files can render directly on GitHub.
We provide several notebooks (like <a href="https://github.com/Luolc/AdaBound/blob/master/demos/cifar10/visualization.ipynb">this one</a>)
for better visualization.
We hope to illustrate the robust performance of <span style="font-variant: small-caps">AdaBound</span> through these examples.</p>

		</div>








<h2 id="bibtex">BibTex</h2>
<pre><code>@inproceedings{Luo2019AdaBound,
  author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  booktitle = {Proceedings of the 7th International Conference on Learning Representations},
  month = {May},
  year = {2019},
  address = {New Orleans, Louisiana}
}
</code></pre>
	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/publications/personalized-goal-oriented-dialog/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">Learning Personalized End-to-End Goal-Oriented Dialog</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/publications/parallel-multi-scale-attention/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning</p></a>
	</div>
</nav>


			</section>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2015-2025 Liangchen Luo.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> based on <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>