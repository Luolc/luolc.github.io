<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Liangchen Luo</title>
    <link>https://www.luolc.com/</link>
    <description>Recent content on Liangchen Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.luolc.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection</title>
      <link>https://www.luolc.com/publications/sparse-rag/</link>
      <pubDate>Thu, 24 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/sparse-rag/</guid>
      <description>Abstract Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts. However, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency. In this paper, we propose a novel paradigm named Sparse RAG, which seeks to cut computation costs through sparsity. Speciﬁcally, Sparse RAG encodes retrieved documents in parallel, which eliminates latency introduced by long-range attention of retrieved documents.</description>
    </item>
    <item>
      <title>Work Experience</title>
      <link>https://www.luolc.com/news/xai/</link>
      <pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/xai/</guid>
      <description>I joined xAI.</description>
    </item>
    <item>
      <title>Paper Accepted</title>
      <link>https://www.luolc.com/news/iclr-2025-accpeted/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/iclr-2025-accpeted/</guid>
      <description>A paper got accepted to ICLR 2025.</description>
    </item>
    <item>
      <title>Fusion-Eval: Integrating Assistant Evaluators with LLMs</title>
      <link>https://www.luolc.com/publications/fusion-eval/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/fusion-eval/</guid>
      <description>Abstract Evaluating natural language generation (NLG) systems automatically poses significant challenges. Recent studies have employed large language models (LLMs) as reference-free metrics for NLG evaluation, enhancing adaptability to new tasks tasks. However, these methods still show lower correspondence with human judgments compared to specialized neural evaluators. In this paper, we introduce “Fusion-Eval”, an innovative approach that leverages LLMs to integrate insights from various assistant evaluators. The LLM is given the example to evaluate along with scores from the assistant evaluators.</description>
    </item>
    <item>
      <title>Multi-Step Problem Solving Through a Verifier: An Empirical Analysis on Model-Induced Process Supervision</title>
      <link>https://www.luolc.com/publications/mips/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/mips/</guid>
      <description>Abstract Process supervision, using a trained verifier to evaluate the intermediate steps generated by a reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid the expensive effort of human annotation on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions.</description>
    </item>
    <item>
      <title>Paper Accepted</title>
      <link>https://www.luolc.com/news/emnlp-2024-accepted/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/emnlp-2024-accepted/</guid>
      <description>Two papers got accepted to EMNLP 2024.</description>
    </item>
    <item>
      <title>Towards an On-Device Agent for Text Rewriting</title>
      <link>https://www.luolc.com/publications/on-device-rewriting/</link>
      <pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/on-device-rewriting/</guid>
      <description>Abstract Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. However creating a smaller yet potent language model for text rewriting presents two formidable challenges: costly data collection and absence of emergent capabilities. In this paper we present solutions to address the above challenges. We propose an new instruction tuning method to develop a mobile text rewriting model that leverages LLM-generated data and heuristic reinforcement learning, eliminating the need for human data collection.</description>
    </item>
    <item>
      <title>Improve Mathematical Reasoning in Language Models by Automated Process Supervision</title>
      <link>https://www.luolc.com/publications/omegaprm/</link>
      <pubDate>Wed, 22 May 2024 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/omegaprm/</guid>
      <description>Abstract Complex multi-step reasoning tasks, such as solving mathematical problems or generating code, remain a significant hurdle for even the most advanced large language models (LLMs). Verifying LLM outputs with an Outcome Reward Model (ORM) is a standard inference-time technique aimed at enhancing the reasoning performance of LLMs. However, this still proves insufficient for reasoning tasks with a lengthy or multi-hop reasoning chain, where the intermediate outcomes are neither properly rewarded nor penalized.</description>
    </item>
    <item>
      <title>Work Experience</title>
      <link>https://www.luolc.com/news/google-deepmind/</link>
      <pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/google-deepmind/</guid>
      <description>I transferred to Google DeepMind.</description>
    </item>
    <item>
      <title>RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting</title>
      <link>https://www.luolc.com/publications/rewritelm/</link>
      <pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/rewritelm/</guid>
      <description>Abstract Large Language Models (LLMs) have demonstrated impressive capabilities in creative tasks such as storytelling and E-mail generation. However, as LLMs are primarily trained on final text results rather than intermediate revisions, it might be challenging for them to perform text rewriting tasks. Most studies in the rewriting tasks focus on a particular transformation type within the boundaries of single sentences. In this work, we develop new strategies for instruction tuning and reinforcement learning to better align LLMs for cross-sentence rewriting tasks using diverse wording and structures expressed through natural languages including 1) generating rewriting instruction data from Wiki edits and public corpus through instruction generation and chain-of-thought prompting; 2) collecting comparison data for reward model training through a new ranking function.</description>
    </item>
    <item>
      <title>Paper Accepted</title>
      <link>https://www.luolc.com/news/naacl-2024-accepted/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/naacl-2024-accepted/</guid>
      <description>A paper got accepted to NAACL 2024.</description>
    </item>
    <item>
      <title>Paper Accepted</title>
      <link>https://www.luolc.com/news/aaai-2024-accepted/</link>
      <pubDate>Sat, 09 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/aaai-2024-accepted/</guid>
      <description>A paper got accepted to AAAI 2024.</description>
    </item>
    <item>
      <title>SiRA: Sparse Mixture of Low Rank Adaptation</title>
      <link>https://www.luolc.com/publications/sira/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/sira/</guid>
      <description>Abstract Parameter Efficient Tuning has been an prominent approach to adapt the Large Language Model to downstream tasks. Most previous works considers adding the dense trainable parameters, where all parameters are used to adapt certain task. We found this less effective empirically using the example of LoRA that introducing more trainable parameters does not help. Motivated by this we investigate the importance of leveraging &amp;ldquo;sparse&amp;rdquo; computation and propose SiRA: sparse mixture of low rank adaption.</description>
    </item>
    <item>
      <title>Critique Ability of Large Language Models</title>
      <link>https://www.luolc.com/publications/criticbench/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/criticbench/</guid>
      <description>Abstract Critical thinking is essential for rational decision-making and problem-solving. This skill hinges on the ability to provide precise and reasoned critiques and is a hallmark of human intelligence. In the era of large language models (LLMs), this study explores the ability of LLMs to deliver accurate critiques across various tasks. We are interested in this topic as a capable critic model could not only serve as a reliable evaluator, but also as a source of supervised signals for model tuning.</description>
    </item>
    <item>
      <title>Bridging the Gap Between Object Detection and User Intent via Query-Modulation</title>
      <link>https://www.luolc.com/publications/query-modularized-detection/</link>
      <pubDate>Fri, 18 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/query-modularized-detection/</guid>
      <description>Abstract When interacting with objects through cameras, or pictures, users often have a specific intent. For example, they may want to perform a visual search. With most object detection models relying on image pixels as their sole input, undesired results are not uncommon. Most typically: lack of a high-confidence detection on the object of interest, or detection with a wrong class label. The issue is especially severe when operating capacity-constrained mobile object detectors on-device.</description>
    </item>
    <item>
      <title>Large-Scale Generative Data-Free Distillation</title>
      <link>https://www.luolc.com/publications/data-free-distillation/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/data-free-distillation/</guid>
      <description>Abstract Knowledge distillation is one of the most popular and effective techniques for knowledge transfer, model compression and semi-supervised learning. Most existing distillation approaches require the access to original or augmented training samples. But this can be problematic in practice due to privacy, proprietary and availability concerns. Recent work has put forward some methods to tackle this problem, but they are either highly time-consuming or unable to scale to large datasets.</description>
    </item>
    <item>
      <title>Image Segmentation via Cellular Automata</title>
      <link>https://www.luolc.com/publications/cellular-automata/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/cellular-automata/</guid>
      <description>Abstract In this paper, we propose a new approach for building cellular automata to solve real-world segmentation problems. We design and train a cellular automaton that can successfully segment high-resolution images. We consider a colony that densely inhabits the pixel grid, and all cells are governed by a randomized update that uses the current state, the color, and the state of the 3×3 neighborhood. The space of possible rules is defined by a small neural network.</description>
    </item>
    <item>
      <title>MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning</title>
      <link>https://www.luolc.com/publications/parallel-multi-scale-attention/</link>
      <pubDate>Sun, 17 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/parallel-multi-scale-attention/</guid>
      <description>Abstract In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures.</description>
    </item>
    <item>
      <title>Work Experience</title>
      <link>https://www.luolc.com/news/google-ai-resident/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/google-ai-resident/</guid>
      <description>I graduated from PKU and joined Google Research.</description>
    </item>
    <item>
      <title>Adaptive Gradient Methods with Dynamic Bound of Learning Rate</title>
      <link>https://www.luolc.com/publications/adabound/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/adabound/</guid>
      <description>Abstract Adaptive optimization methods such as AdaGrad, RMSProp and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with Sgd or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods.</description>
    </item>
    <item>
      <title>Learning Personalized End-to-End Goal-Oriented Dialog</title>
      <link>https://www.luolc.com/publications/personalized-goal-oriented-dialog/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/personalized-goal-oriented-dialog/</guid>
      <description>Abstract Most existing works on dialog systems only consider conversation content while neglecting the personality of the user the bot is interacting with, which begets several unsolved issues. In this paper, we present a personalized end-to-end model in an attempt to leverage personalization in goal-oriented dialogs. We first introduce a Profile Model which encodes user profiles into distributed embeddings and refers to conversation history from other similar users. Then a Preference Model captures user preferences over knowledge base entities to handle the ambiguity in user requests.</description>
    </item>
    <item>
      <title>Text Assisted Insight Ranking Using Context-Aware Memory Network</title>
      <link>https://www.luolc.com/publications/insight-ranking/</link>
      <pubDate>Wed, 30 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/insight-ranking/</guid>
      <description>Abstract Extracting valuable facts or informative summaries from multi-dimensional tables, i.e. insight mining, is an important task in data analysis and business intelligence. However, ranking the importance of insights remains a challenging and unexplored task. The main challenge is that explicitly scoring an insight or giving it a rank requires a thorough understanding of the tables and costs a lot of manual efforts, which leads to the lack of available training data for the insight ranking problem.</description>
    </item>
    <item>
      <title>Paper Accepted</title>
      <link>https://www.luolc.com/news/iclr-2019-accpeted/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/iclr-2019-accpeted/</guid>
      <description>A paper got accepted to ICLR 2019.</description>
    </item>
    <item>
      <title>An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation</title>
      <link>https://www.luolc.com/publications/auto-encoder-matching-dialog/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/publications/auto-encoder-matching-dialog/</guid>
      <description>Abstract Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module.</description>
    </item>
    <item>
      <title>Paper Accepted</title>
      <link>https://www.luolc.com/news/aaai-2019-accepted/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/aaai-2019-accepted/</guid>
      <description>Two papers got accepted to AAAI 2019.</description>
    </item>
    <item>
      <title>Award</title>
      <link>https://www.luolc.com/news/award-2018-liaokaiyuan/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/award-2018-liaokaiyuan/</guid>
      <description>I was awarded with Liao Kaiyuan Scholarship and Academic Excellence Award!</description>
    </item>
    <item>
      <title>Paper Accepted</title>
      <link>https://www.luolc.com/news/emnlp-2018-accepted/</link>
      <pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/emnlp-2018-accepted/</guid>
      <description>A paper got accepted to EMNLP 2018.</description>
    </item>
    <item>
      <title>Internship</title>
      <link>https://www.luolc.com/news/didi-internship/</link>
      <pubDate>Thu, 26 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://www.luolc.com/news/didi-internship/</guid>
      <description>Started as a research assistant at DiDi AI Labs.</description>
    </item>
  </channel>
</rss>
